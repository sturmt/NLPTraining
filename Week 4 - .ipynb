{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMlPzmeAZTEUjXs1ONyUIsB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sturmt/NLPtraining/blob/main/Week%204%20-%20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLXWyWY9g3ED",
        "outputId": "2a270336-a124-489e-dfc9-c19581e1767d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import re\n",
        "from sklearn.preprocessing import normalize\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "oWOQAI0zhIcV"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(doc):\n",
        "    tokenized_doc = word_tokenize(doc.lower())\n",
        "    cleaned_doc = \" \".join([lemmatizer.lemmatize(token) for token in tokenized_doc if not token in stop_words and token.isalpha()])\n",
        "    return cleaned_doc\n",
        "\n",
        "doc_1 = \"The man went to the moon in 1969 and walked on it\"\n",
        "doc_2 = \"Arnold at the same time was winning bodybuilding\"\n",
        "doc_3 = \"Tom was not born yet during this amazing time\"\n",
        "\n",
        "corpus = [doc_1, doc_2, doc_3]\n",
        "\n",
        "preprocess_corpus = [preprocess_text(doc) for doc in corpus]\n",
        "\n",
        "#initialize\n",
        "vectorizer = TfidfVectorizer(norm=None)\n",
        "\n",
        "#\n",
        "tf_idf_scores = vectorizer.fit_transform(preprocess_corpus)\n",
        "\n",
        "#need to get vocabulary from the terms we had\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "#use corpus index\n",
        "corpus_index = [f\"Document {i+1}\" for i in range(len(corpus))]\n",
        "\n",
        "#use pandas for ??\n",
        "df_tf_idf = pd.DataFrame(tf_idf_scores.T.todense(), index= feature_names, columns=corpus_index)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYtTqDuEg83F",
        "outputId": "0ddd9590-d597-4c40-d90c-b2f35707a0f4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "WCJFNIdimLMq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CountVectorizer,\n",
        "#function to clean up text\n",
        "def preprocess_text(text):\n",
        "    #change to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    #remove punctution\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    #remove extra white spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "poem = '''\n",
        "Success is counted sweetest\n",
        "By those who ne'er succeed.\n",
        "To comprehend a nectar\n",
        "Requires sorest need.\n",
        "\n",
        "Not one of all the purple host\n",
        "Who took the flag to-day\n",
        "Can tell the definition,\n",
        "So clear, of victory,\n",
        "\n",
        "As he, defeated, dying,\n",
        "On whose forbidden ear\n",
        "The distant strains of triumph\n",
        "Break, agonized and clear!'''\n",
        "\n",
        "processed_poem = preprocess_text(poem)\n",
        "\n",
        "#init count vector\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Note that fit_transform expects a list of strings.\n",
        "term_frequencies = vectorizer.fit_transform([processed_poem])\n",
        "\n",
        "#get vocabulary of terms\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "#create Pandas DataFrame with term frequency\n",
        "try:\n",
        "    df_term_frequencies = pd.DataFrame(term_frequencies.T.todense(), index=feature_names, columns=[\"Term Frequency\"])\n",
        "    print(df_term_frequencies)\n",
        "except:\n",
        "    pass\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UE7yq9G1mNga",
        "outputId": "2055886b-b42d-4942-a01c-99c99554331c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            Term Frequency\n",
            "agonized                 1\n",
            "all                      1\n",
            "and                      1\n",
            "as                       1\n",
            "break                    1\n",
            "by                       1\n",
            "can                      1\n",
            "clear                    2\n",
            "comprehend               1\n",
            "counted                  1\n",
            "defeated                 1\n",
            "definition               1\n",
            "distant                  1\n",
            "dying                    1\n",
            "ear                      1\n",
            "flag                     1\n",
            "forbidden                1\n",
            "he                       1\n",
            "host                     1\n",
            "is                       1\n",
            "nectar                   1\n",
            "need                     1\n",
            "neer                     1\n",
            "not                      1\n",
            "of                       3\n",
            "on                       1\n",
            "one                      1\n",
            "purple                   1\n",
            "requires                 1\n",
            "so                       1\n",
            "sorest                   1\n",
            "strains                  1\n",
            "succeed                  1\n",
            "success                  1\n",
            "sweetest                 1\n",
            "tell                     1\n",
            "the                      4\n",
            "those                    1\n",
            "to                       1\n",
            "today                    1\n",
            "took                     1\n",
            "triumph                  1\n",
            "victory                  1\n",
            "who                      2\n",
            "whose                    1\n"
          ]
        }
      ]
    }
  ]
}